---
title: LLM Integration
description: AI SDK integration and direct model access patterns
---

# LLM Integration

## Vercel AI SDK Integration (Recommended)

The Echo React SDK provides first-class integration with the Vercel AI SDK for the best developer experience.

### useEchoModelProviders Hook

Get AI SDK-compatible providers for OpenAI, Anthropic, and Google models:

```tsx {21}
import { useEchoModelProviders } from '@merit-systems/echo-react-sdk';
import { generateText } from 'ai';

function AIComponent() {
  const { openai, anthropic, google } = useEchoModelProviders();
  
  const handleGenerate = async () => {
    // Use any provider with the AI SDK
    const { text } = await generateText({
      model: await openai('gpt-5'),
      prompt: 'Write a haiku about coding'
    });
    
    console.log(text);
  };
  
  return <button onClick={handleGenerate}>Generate Text</button>;
}
```

### Streaming Example

Use [`streamText`](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text) from the Vercel AI SDK for real-time text generation:

```tsx {8,16-18}
import { streamText } from 'ai';

function StreamingGenerator() {
  const { openai } = useEchoModelProviders();
  const [result, setResult] = useState('');
  
  const handleStream = async () => {
    const { textStream } = await streamText({
      model: await openai('gpt-5'),
      prompt: 'Write a story about space exploration'
    });
    
    setResult(''); // Clear previous result
    
    // Stream tokens as they arrive
    for await (const delta of textStream) {
      setResult(prev => prev + delta);
    }
  };
  
  return (
    <div>
      <button onClick={handleStream}>Start Streaming</button>
      <div style={{ whiteSpace: 'pre-wrap' }}>{result}</div>
    </div>
  );
}
```

## Custom Chat Hook

For advanced chat interfaces, see our [dedicated useChat documentation](/docs/react-sdk/chat).

## Raw Client Access (Advanced)

For lower-level control, you can create raw clients with your JWT token:

### Mental Model: JWT + Base URL

Understanding how Echo enables direct LLM calls:

```tsx
import { OpenAI } from 'openai';

function RawOpenAIExample() {
  const { token } = useEcho();
  
  // This is what happens under the hood
  const openai = new OpenAI({
    apiKey: token, // Your JWT token acts as the API key
    baseURL: 'https://echo.router.merit.systems', // Echo's proxy endpoint
    dangerouslyAllowBrowser: true // Safe because token is user-scoped
  });
  
  const handleChat = async () => {
    const response = await openai.chat.completions.create({
      model: 'gpt-5',
      messages: [{ role: 'user', content: 'Hello!' }]
    });
    
    return response.choices[0].message.content;
  };
  
  return <button onClick={handleChat}>Raw Client Call</button>;
}
```


