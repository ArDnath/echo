---
title: LLM Integration
description: Provider usage with automatic token management and Vercel AI SDK integration
---

import { Callout } from 'fumadocs-ui/components/callout';

# LLM Integration

Echo Next.js SDK provides async LLM providers that integrate seamlessly with Vercel AI SDK patterns while handling authentication automatically.

## Provider Architecture

### Async Provider Pattern

All providers return Promises for automatic token management:

```typescript
import Echo from '@merit-systems/echo-next-sdk';

const { openai, anthropic } = Echo({ appId: 'your-echo-app-id' });

// All provider methods are async
const openaiProvider = await openai();
const anthropicProvider = await anthropic();

// Model-specific providers
const gpt4 = await openai('gpt-4');
const claude = await anthropic('claude-4-sonnet');
```

<Callout type="info">
The async pattern enables automatic token refresh before each LLM call, ensuring requests never fail due to expired authentication.
</Callout>

### Available Providers

**OpenAI Provider:**
```typescript
const openaiProvider = await openai();           // Full provider
const gpt4Model = await openai('gpt-4');         // Specific model
```

**Anthropic Provider:**
```typescript  
const anthropicProvider = await anthropic();     // Full provider
const claudeModel = await anthropic('claude-4-sonnet'); // Specific model
```

## Vercel AI SDK Integration

### Streaming Text Generation

```typescript title="app/api/chat/route.ts"
import Echo from '@merit-systems/echo-next-sdk';
import { streamText } from 'ai';

const { openai } = Echo({ appId: 'your-echo-app-id' });

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  const result = streamText({
    model: await openai('gpt-4'),
    messages,
    temperature: 0.7,
  });
  
  return result.toUIMessageStreamResponse();
}
```

### Object Generation

```typescript title="app/api/extract/route.ts"
import { openai } from '@merit-systems/echo-next-sdk';
import { generateObject } from 'ai';
import { z } from 'zod';

const schema = z.object({
  name: z.string(),
  age: z.number(),
  location: z.string(),
});

export async function POST(req: Request) {
  const { text } = await req.json();
  
  const result = await generateObject({
    model: await openai('gpt-4'),
    prompt: `Extract person info from: ${text}`,
    schema,
  });
  
  return Response.json(result.object);
}
```

### Text Generation

```typescript title="app/api/generate/route.ts"
import { anthropic } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export async function POST(req: Request) {
  const { prompt } = await req.json();
  
  const { text } = await generateText({
    model: await anthropic('claude-4-sonnet'),
    prompt,
    maxTokens: 1000,
  });
  
  return Response.json({ text });
}
```

## Server Component Integration

### Direct LLM Calls in RSC

```typescript title="app/analysis/page.tsx"
import { openai } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export default async function AnalysisPage({
  searchParams
}: {
  searchParams: { query?: string }
}) {
  if (!searchParams.query) {
    return <div>Enter a query to analyze</div>;
  }
  
  const { text } = await generateText({
    model: await openai('gpt-4'),
    prompt: `Analyze this topic: ${searchParams.query}`,
  });
  
  return (
    <div>
      <h1>Analysis Results</h1>
      <p>{text}</p>
    </div>
  );
}
```

### Parallel LLM Calls

```typescript title="app/comparison/page.tsx"
import { openai, anthropic } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export default async function ComparisonPage({
  searchParams
}: {
  searchParams: { topic?: string }
}) {
  const topic = searchParams.topic;
  if (!topic) return <div>No topic provided</div>;
  
  // Parallel calls to different providers
  const [openaiResponse, anthropicResponse] = await Promise.all([
    generateText({
      model: await openai('gpt-4'),
      prompt: `Explain ${topic} from a technical perspective`,
    }),
    generateText({
      model: await anthropic('claude-4-sonnet'), 
      prompt: `Explain ${topic} from a business perspective`,
    })
  ]);
  
  return (
    <div className="grid grid-cols-2 gap-4">
      <div>
        <h2>Technical (GPT-4)</h2>
        <p>{openaiResponse.text}</p>
      </div>
      <div>
        <h2>Business (Claude)</h2>
        <p>{anthropicResponse.text}</p>
      </div>
    </div>
  );
}
```

## Server Actions

### Form Processing with LLM

```typescript title="app/summarize/page.tsx"
import { openai } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

async function summarizeText(formData: FormData) {
  'use server';
  
  const text = formData.get('text') as string;
  
  const { text: summary } = await generateText({
    model: await openai('gpt-4'),
    prompt: `Summarize this text in 2-3 sentences: ${text}`,
  });
  
  return { summary };
}

export default function SummarizePage() {
  return (
    <form action={summarizeText}>
      <textarea 
        name="text" 
        placeholder="Enter text to summarize..."
        className="w-full h-32 p-2 border rounded"
      />
      <button type="submit" className="mt-2 px-4 py-2 bg-blue-600 text-white rounded">
        Summarize
      </button>
    </form>
  );
}
```

### Progressive Enhancement

```typescript title="app/translate/actions.ts"
'use server';
import { openai } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export async function translateText(text: string, targetLanguage: string) {
  const { text: translation } = await generateText({
    model: await openai('gpt-4'),
    prompt: `Translate this to ${targetLanguage}: ${text}`,
  });
  
  return translation;
}
```

```typescript title="app/translate/page.tsx"
import { translateText } from './actions';

export default function TranslatePage() {
  return (
    <form action={async (formData: FormData) => {
      'use server';
      const text = formData.get('text') as string;
      const language = formData.get('language') as string;
      const result = await translateText(text, language);
      console.log('Translation:', result);
    }}>
      <input name="text" placeholder="Text to translate" />
      <select name="language">
        <option value="Spanish">Spanish</option>
        <option value="French">French</option>
        <option value="German">German</option>
      </select>
      <button type="submit">Translate</button>
    </form>
  );
}
```

## Direct Provider API Access

### OpenAI Direct API

```typescript title="app/api/openai-direct/route.ts"
import { openai } from '@merit-systems/echo-next-sdk';

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  // Get OpenAI provider instance
  const provider = await openai();
  
  const response = await provider.chat.completions.create({
    model: 'gpt-4',
    messages,
    temperature: 0.7,
    max_tokens: 1000,
  });
  
  return Response.json(response);
}
```

### Anthropic Direct API  

```typescript title="app/api/anthropic-direct/route.ts"
import { anthropic } from '@merit-systems/echo-next-sdk';

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  // Get Anthropic provider instance
  const provider = await anthropic();
  
  const response = await provider.messages.create({
    model: 'claude-4-sonnet',
    max_tokens: 1000,
    messages,
  });
  
  return Response.json(response);
}
```

## Streaming Responses

### UI Streaming with useChat

```typescript title="app/api/chat/route.ts"
import { openai } from '@merit-systems/echo-next-sdk';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  const result = streamText({
    model: await openai('gpt-4'),
    messages,
  });
  
  return result.toUIMessageStreamResponse();
}
```

```typescript title="app/chat/page.tsx"
'use client';
import { useChat } from 'ai/react';

export default function ChatPage() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat',
  });
  
  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <strong>{message.role}:</strong> {message.content}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={handleInputChange} />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}
```

### Custom Streaming

```typescript title="app/api/stream-custom/route.ts"
import { openai } from '@merit-systems/echo-next-sdk';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { prompt } = await req.json();
  
  const result = streamText({
    model: await openai('gpt-4'),
    prompt,
  });
  
  // Stream to custom format
  const stream = new ReadableStream({
    async start(controller) {
      for await (const chunk of result.textStream) {
        controller.enqueue(`data: ${JSON.stringify({ text: chunk })}\n\n`);
      }
      controller.close();
    },
  });
  
  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
    },
  });
}
```

## Error Handling

### Provider Error Handling

```typescript
import { openai } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export async function POST(req: Request) {
  try {
    const result = await generateText({
      model: await openai('gpt-4'),
      prompt: 'Hello world',
    });
    
    return Response.json({ text: result.text });
  } catch (error) {
    // Handle authentication errors
    if (error.message?.includes('authentication')) {
      return Response.json({ error: 'Please sign in' }, { status: 401 });
    }
    
    // Handle LLM provider errors
    if (error.message?.includes('rate limit')) {
      return Response.json({ error: 'Rate limited' }, { status: 429 });
    }
    
    return Response.json({ error: 'Request failed' }, { status: 500 });
  }
}
```

### Authentication Error Recovery

```typescript
import { openai } from '@merit-systems/echo-next-sdk';
import { isSignedIn } from '@merit-systems/echo-next-sdk';

export async function safeLLMCall(prompt: string) {
  // Check authentication first
  if (!(await isSignedIn())) {
    throw new Error('Not authenticated');
  }
  
  try {
    const model = await openai('gpt-4');
    const result = await generateText({ model, prompt });
    return result.text;
  } catch (error) {
    // Re-check authentication on failure
    if (!(await isSignedIn())) {
      throw new Error('Authentication expired');
    }
    throw error;
  }
}
```

## Advanced Patterns

### Model Fallback Strategy

```typescript title="lib/llm-fallback.ts"
import { openai, anthropic } from '@merit-systems/echo-next-sdk';
import { generateText } from 'ai';

export async function generateWithFallback(prompt: string) {
  try {
    // Try GPT-4 first
    return await generateText({
      model: await openai('gpt-4'),
      prompt,
    });
  } catch (error) {
    console.warn('GPT-4 failed, falling back to Claude:', error);
    
    // Fallback to Claude
    return await generateText({
      model: await anthropic('claude-4-sonnet'),
      prompt,
    });
  }
}
```

### Provider Configuration

```typescript title="lib/llm-config.ts"
import { openai, anthropic } from '@merit-systems/echo-next-sdk';

export async function getModelForTask(task: 'chat' | 'code' | 'analysis') {
  switch (task) {
    case 'chat':
      return await openai('gpt-4');
    case 'code': 
      return await openai('gpt-4');
    case 'analysis':
      return await anthropic('claude-4-sonnet');
    default:
      return await openai('gpt-4');
  }
}
```

<Callout type="info">
All provider calls automatically handle token refresh. Failed authentication indicates the user needs to re-authenticate through the OAuth flow.
</Callout>